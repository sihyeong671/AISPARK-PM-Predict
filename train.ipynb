{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m커널을 시작하지 못했습니다. \n",
      "\u001b[1;31mJupyter server crashed. Unable to connect. \n",
      "\u001b[1;31mError code from Jupyter: 1\n",
      "\u001b[1;31musage: jupyter.py [-h] [--version] [--config-dir] [--data-dir] [--runtime-dir]\n",
      "\u001b[1;31m                  [--paths] [--json] [--debug]\n",
      "\u001b[1;31m                  [subcommand]\n",
      "\u001b[1;31m\n",
      "\u001b[1;31mJupyter: Interactive Computing\n",
      "\u001b[1;31m\n",
      "\u001b[1;31mpositional arguments:\n",
      "\u001b[1;31m  subcommand     the subcommand to launch\n",
      "\u001b[1;31m\n",
      "\u001b[1;31moptions:\n",
      "\u001b[1;31m  -h, --help     show this help message and exit\n",
      "\u001b[1;31m  --version      show the versions of core jupyter packages and exit\n",
      "\u001b[1;31m  --config-dir   show Jupyter config dir\n",
      "\u001b[1;31m  --data-dir     show Jupyter data dir\n",
      "\u001b[1;31m  --runtime-dir  show Jupyter runtime dir\n",
      "\u001b[1;31m  --paths        show all Jupyter paths. Add --json for machine-readable\n",
      "\u001b[1;31m                 format.\n",
      "\u001b[1;31m  --json         output paths as machine-readable json\n",
      "\u001b[1;31m  --debug        output debug information about paths\n",
      "\u001b[1;31m\n",
      "\u001b[1;31mAvailable subcommands:\n",
      "\u001b[1;31m\n",
      "\u001b[1;31mJupyter command `jupyter-notebook` not found. \n",
      "\u001b[1;31m자세한 내용은 Jupyter <a href='command:jupyter.viewOutput'>로그</a>를 참조하세요."
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "from sklearn import preprocessing\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "  def __init__(self, X, y):\n",
    "    self.X = X\n",
    "    self.y = y\n",
    "        \n",
    "  def __getitem__(self, index):\n",
    "    return self.X[index], self.y[index]\n",
    "    \n",
    "  def __len__(self):\n",
    "    return len(self.X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM(nn.Module):\n",
    "  def __init__(self, num_classes, num_layers, input_size, hidden_size, device):\n",
    "    super().__init__()\n",
    "    self.num_classes = num_classes\n",
    "    self.num_layer = num_layers\n",
    "    self.input_size = input_size\n",
    "    self.hidden_size = hidden_size\n",
    "    self.device = device\n",
    "    \n",
    "    self.lstm = nn.LSTM(\n",
    "      input_size=input_size,\n",
    "      hidden_size=hidden_size,\n",
    "      num_layers=num_layers,\n",
    "      batch_first=True,\n",
    "      dropout=0.2)\n",
    "    self.fc_1 = nn.Linear(hidden_size, 128)\n",
    "    self.fc_2 = nn.Linear(128, num_classes)\n",
    "    self.relu = nn.ReLU(inplace=True)\n",
    "    \n",
    "  def forward(self, x):\n",
    "    h_0 = torch.zeros(self.num_layer, x.size(0), self.hidden_size, device=self.device)\n",
    "    c_0 = torch.zeros(self.num_layer, x.size(0), self.hidden_size, device=self.device)\n",
    "    \n",
    "    output, (hn, cn) = self.lstm(x, (h_0, c_0))\n",
    "    hn = hn.view(-1, self.hidden_size)\n",
    "    out = self.relu(hn)\n",
    "    out = self.fc_1(out)\n",
    "    out = self.relu(out)\n",
    "    out = self.fc_2(out)\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Trainer:\n",
    "  def __init__(self, path):\n",
    "    self.path = path\n",
    "    self.n_step_in = 24 * 2\n",
    "    self.n_step_out = 24 * 3\n",
    "    \n",
    "    self.n_epochs = 100\n",
    "    self.lr = 0.001\n",
    "    self.input_size = 6\n",
    "    self.hidden_size = 8\n",
    "    self.num_layers = 1\n",
    "    self.num_classes = 24*3\n",
    "    \n",
    "    self.device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "    \n",
    "  def set_up(self):\n",
    "    path = './dataset/CUSTOM_v1/'\n",
    "    csv_list = os.listdir(path)\n",
    "    X_train=[], X_val=[], y_train=[], y_val=[]\n",
    "    for file in csv_list:\n",
    "      df = pd.read_csv(path+str(file))\n",
    "      df.drop(columns=[\"연도\", \"일시\", \"측정소\"], inplace=True)\n",
    "\n",
    "      scaler = preprocessing.MinMaxScaler()\n",
    "      scaled_np = scaler.fit_transform(df.values)\n",
    "      scaled_df = pd.DataFrame(scaled_np, columns=df.columns)\n",
    "      X_tmp, y_tmp = self._split_sequences(\n",
    "        input_seq=scaled_df.values,\n",
    "        output_seq=scaled_df[\"PM2.5\"].values,\n",
    "        n_step_in=self.n_step_in,\n",
    "        n_step_out=self.n_step_out)\n",
    "      \n",
    "      X_train_tmp = X_tmp[:-3000]\n",
    "      X_val_tmp = X_tmp[-3000:]\n",
    "      \n",
    "      y_train_tmp = y_tmp[:-3000]\n",
    "      y_val_tmp = y_tmp[-3000:]\n",
    "\n",
    "      X_train.append(X_train_tmp)\n",
    "      X_val.append(X_val_tmp)\n",
    "      y_train.append(y_train_tmp)\n",
    "      y_val.append(y_val_tmp)\n",
    "      \n",
    "    train_dataset = CustomDataset(X_train, y_train)\n",
    "    val_dataset = CustomDataset(X_val, y_val)\n",
    "\n",
    "    self.train_dataloader = DataLoader(\n",
    "      dataset=train_dataset,\n",
    "      shuffle=True,\n",
    "      batch_size=128,\n",
    "    )\n",
    "    \n",
    "    self.val_dataloader = DataLoader(\n",
    "      dataset=val_dataset,\n",
    "      shuffle=False,\n",
    "      batch_size=256,\n",
    "    )\n",
    "    \n",
    "  def _split_sequences(self, input_seq, output_seq, n_step_in, n_step_out):\n",
    "    X, y = list(), list()\n",
    "    for i in range(len(input_seq)):\n",
    "      end_idx = i + n_step_in\n",
    "      out_end_idx = end_idx + n_step_out\n",
    "      if out_end_idx > len(input_seq):\n",
    "        break\n",
    "      seq_x, seq_y = input_seq[i:end_idx], output_seq[end_idx:out_end_idx]\n",
    "      X.append(seq_x)\n",
    "      y.append(seq_y)\n",
    "    return np.array(X), np.array(y)\n",
    "  \n",
    "\n",
    "  def train(self):\n",
    "    model = LSTM(\n",
    "      num_classes=self.num_classes,\n",
    "      num_layers=self.num_layers,\n",
    "      input_size=self.input_size,\n",
    "      hidden_size=self.hidden_size,\n",
    "      device=self.device).to(self.device)\n",
    "    \n",
    "    loss_fn = nn.MSELoss().to(self.device)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=self.lr)\n",
    "    \n",
    "    print(\"TRAIN START\")\n",
    "    for epoch in range(1, self.n_epochs+1):\n",
    "      model.train()\n",
    "      train_loss = []\n",
    "      for batch in self.train_dataloader:\n",
    "        X, y = batch\n",
    "        X = X.float().to(self.device)\n",
    "        y = y.float().to(self.device)\n",
    "        outputs = model.forward(X)\n",
    "        optimizer.zero_grad()\n",
    "        loss = loss_fn(outputs, y)\n",
    "        train_loss.append(loss.item())\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "      \n",
    "      model.eval()\n",
    "      with torch.no_grad():\n",
    "        val_loss = []\n",
    "        for batch in self.val_dataloader:\n",
    "          X, y = batch\n",
    "          # numpy 기본 자료형은 float64이므로 float32에 매칭시켜줘야함\n",
    "          X = X.float().to(self.device)\n",
    "          y = y.float().to(self.device)\n",
    "          outputs = model.forward(X)\n",
    "          loss = loss_fn(outputs, y)\n",
    "          val_loss.append(loss.item())\n",
    "      \n",
    "      \n",
    "      if epoch % 10 == 0:\n",
    "        # 데이터에 nan값이 있는 경우 loss가 계속 nan값이 나옴\n",
    "        print(f\"Epoch: {epoch},\\\n",
    "          train loss: {sum(train_loss)/len(train_loss):.5f},\\\n",
    "          val loss: {sum(val_loss)/len(val_loss):.5f}\")\n",
    "        \n",
    "    print(\"TRAIN FIN\")\n",
    "\n",
    "    return model\n",
    "  \n",
    "      \n",
    "  def plot(self, model):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "      X, y = next(iter(self.val_dataloader))\n",
    "      X = X.float().to(self.device)\n",
    "      y = y.float().to(self.device)\n",
    "      output = model.forward(X)\n",
    "      print(output.shape)\n",
    "      print(y.shape)\n",
    "    y = y.cpu().numpy()\n",
    "    output = output.cpu().numpy()\n",
    "    plt.plot(output[0].squeeze(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(path=\"./dataset/CUSTOM_v0/공주.csv\")\n",
    "trainer.set_up()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mdl = trainer.train()\n",
    "torch.save(mdl, \"lstm.pth\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ca34cd2f0fed7b47a0c67361fc349fbebd3fc19e9e2e994efb32a5976e8044a1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
